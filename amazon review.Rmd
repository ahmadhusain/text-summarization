---
title: "Sequence to Sequence Model"
author: "Ahmad Husain Abdullah"
date: "December 26, 2018"
output: 
  html_document:
    toc: true
    highlight: breezedark
    theme: united
    toc_float: true
    df_print: paged
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  eval = FALSE,
  fig.align = "center"
)
```

In this article, We will go through explanation about sequence to sequence model. After completing this tutorial, you will know:

* Quick review why Recurrent Neural Network instead of Vanilla Neuaral Network for sequential set data.
* Sequence to sequence model implementation in a various business.
* Many type of text data vectorization in sequence to sequence model.
* The different of Abstractive and Extractive summarization.
* How to build encoder-decoder architecture for text summarization.

I recommend that you have (prerequisites):

* Familiarity with R programming
* Familiarity with neural network and in particular, Recurrent Neural Networks (RNNs). If you are not yet familiear with RNNs, I recommend reading the RNN section which will give you quick start. For you who want to dive deep to RNN architecture please look up to [this article](https://algotech.netlify.com/blog/text-lstm/).

# Text Summarization

*Text summarization* is a branch in natural language processing (NLP) to generate a shorter and concise version of a text while preserving the meaning of the original text. Text summarization is nowadays has been applied in a lot of business, for example, news websites that use *text summarization* to give the readers an overview of what a particular article talk about.

> **Goal**: reducing a text with a computer program in order to create summary that retain the most important points of the original text.

Automatic text summarization is very challenging, because we as humans summarize a piece of text, we usually read it entirely to develop our understanding, and then write a summary highlighting its main point. Since computers lack human knowledge and language capability, it make text summarization a very difficult and non-trivial task[^2].

Based on a paper by [Gaikwad, D (2016)](https://pdfs.semanticscholar.org/0681/b372590f7cccc049d24bbddcfb31b68cce61.pdf), text summarizers identify and extract key sentence from the source text and concatenat them to form a concise summary. Below is a list of feature for text summarization [^3].

* **Term Frequency**: The frequently occurring word increases score of sentences. The most common measure widely used to calculate the word frequency is *Term Fequence Inverse Document Frequence* (TF IDF).
* **Location**: It depends on the intuition that inportant sentences are located at certain position in text or in paragraph. First and last sentence of paragraph has greater chance to be included in summary.
* **Cue Method**: Effect of positive or negativity of word on the sentence weight to indicate importance or key idea such as cues: *'in summary'*, *'in conclusion'*, *'the paper describes'*.
* **Title/Headline**: Words in the title and heading of document that occur in sentences are positively related to summarization. Words that appear in the title are also indicative of the topic or subject of the document.
* **Sentence length**: Keeps in view the size of summary. Generally, very long and very short sentences are also not suitable for summary.
* **Similarity**: Similarity can be calculated with linguistic knowledge. it indicated similarity between the sentence antitle of the document, and similarity between the sentence and remaining sentence of the docment. 
* **Proper noun**: For document summarization sentences having proper nouns are important. Like name of a person, place or organizaation.
* **Proximity**: The distance berween text units where entities occur is determining factor for establishing relation between entities.

## Text Summarization Method

In general, there are two difference approaches for text summarization:

1. Extractive summarization
2. Abstractive summarization

### Extractive Method

Extracting text summarization algoritmh are capable of *extracting* key sentence from a text without modifying any word. They depend only on extraction of sentences from the original text. 

> Selecting set of sentences from the source text, then arranging them to form a summary.

```{r echo=FALSE, out.width="60%", eval=TRUE, fig.cap="Fig 1. Extractive Summarization Idea"}
knitr::include_graphics(path = "assets/extractive.png")
```

Methods:

1. Lhun 
2. TextRank
3. SumBasic
4. Edmunson
5. LexRank
6. Latent Semantic Analysis

### Abstractive Methods

Abstractive methods need a deeper analysis of the text. These methods have ability to generate a new sentance, which improves the context of a summary, reduce its redundancy and keep a good compression rate.

```{r echo=FALSE, out.width="60%", eval=TRUE, fig.cap="Fig 2. Abstractive Summarization Idea"}
knitr::include_graphics(path = "assets/abstractive.png")
```

Methods:

1. **Sequence to Sequence Model**
2. Sequence to Sequence with attention
3. Pointer Generation Network
4. Fast Abstractive Reinforcement

On this article we more focused on abstractive methods with RNN sequence to sequence model:

# Sequence to Sequence Method

A lot of algorithms for both extractive and abstractive text summarization are based on Recurrent Neural Networks(RNN). Furthermore, using RNNs in an Encoder-Decoder manner leads us to the well known Sequence-To-Sequence (Seq2Seq) architecture [^1]. A sequence to sequence model lies to numerous system which we face on a daily basis. For example, seq2seq model powers application like *Google Translate* (machine translation), voice-enabled device (speech recognation), video captioning, and online chatbots.

# Recurrent Neural Network

We know that neural network uses an algorithm called **Backpropagation** to update the weights of the network. So what Backpropogation does is it first calculates the gradients from the error using the chain rule, then in updates the weights (*Gradient Descent*). 

When doing backpropogation in simple neural network (1 hidden layer) we might not encounter the problem update weights. But....

> When we build an architecture with a large number of hidden layer (Deep Neural Network) the model is likely to encounter update weight problem called *vanishing* / *exploding* gradient descent.

**Vanishing Gradient Descent**: the update value obtained will exponentially decrease when heading to the input layer. Here are the illustrations, which I took from: [Michael Nguyen's article](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)

```{r echo=FALSE, eval = TRUE, out.width='35%'}
knitr::include_graphics('assets/vg1.gif')
```

Gradient descent aims to adjust weights that allow the model to '**learn**'. the nature of the gradient that is, the greater the value of the gradient in the current layer, will affect in the next layer getting bigger. and *vice versa*. This is the problem. When doing BP, each node will calculate the gradient value and update its weight according to the gradient effect on the previous layer. so if the previous layer is small, then adjusting the weights in the current layer will be small. it causes the gradient to shrink exponentially when it goes to the input layer. so that when in the input layer it fails to do the learning due to *vanishing gradient problems*. so the model **fails to learn** when a pass forward is made again to make predictions.

**Exploding Gradient Descent**: the update value obtained will exponentially increase when heading to the input layer. The characteristics of the model have an exploding gradient problem, which is when the cost function results are NaN. 

```{r echo=FALSE, eval = TRUE, out.width='80%'}
knitr::include_graphics('assets/lossnan.png')
```

> From the vasishing / exploding gradient problem mentioned above, the development of architecture from the RNN, namely LSTM and GRU, is able to handle the problem. *will be discussed below*. RNN itself has not been able to handle vanishing gradients due to short-term memory problems.


## What is Seq2Seq method?

Introduced for the first time in 2014 by [Google](https://arxiv.org/pdf/1409.3215.pdf), a sequence to sequence model aims to map a fixed-length input with a fixed length output where the length of input and output may differ. For example, translating "*How are you?*" from English to Indonesian has input of 3 words words and output 2 words "*Apa kabar?*". So, we can't use a reguler LSTM network to map each word from the English sentence to the Indonesian sentence. This is why sequence to sequence model is used to address the problem like that.

## Encoder-Decoder Model

The most common architecture use to build Seq2Seq models is the Encoder Decoder architecture. In order to understand the architecture’s underlying logic, we will go over the below illustration:

```{r echo=FALSE, out.width="60%", eval=TRUE, fig.cap="Fig 3. Encoder-decoder sequence to sequence model"}
knitr::include_graphics(path = "assets/encoder-decoder.png")
```

Above, a simple Seq2Seq model consists of three parts, Encoder-LSTM, Decoder-LSTM, and State also known as the context.

Point to note:

* Both encoder and decoder are RNN models (more often LSTM).
* **Encoder**: The encoder reads the entire input sequence and encodes it into an internal representation, often a fixed-length vector called the context vector.
* **Decoder**: The decoder reads the encoded input sequence from the encoder and generates the output sequence.
* The decoder behaves a bit differently during the training and inference procedure. During the training, we use a technique call teacher forcing which helps to train the decoder faster. During inference, the input to the decoder at each time step is the output from the previous time step.

> ...RNN Encoder-Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence [^4].

```{r echo=FALSE, out.width="80%", eval=TRUE, fig.cap="Fig 4. Encoder-decoder example architecture"}
knitr::include_graphics(path = "assets/example.png")
```

Now, we will understand all the above steps, taken from [this article](https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7). We will keep this section intuitive without going to mathematical detail. So here is the main idea of encoder architecture:

```{r echo=FALSE, out.width="60%", eval=TRUE, fig.cap="Fig 5. Encoder model"}
knitr::include_graphics(path = "assets/encoder.png")
```

The LSTM reads the data one sequence after the other. Thus if the input is a sequence of length ‘k’, we say that LSTM reads it in ‘k’ time steps. Because we have sentences with 3 words, "How",  "are",  you" the LSTM will read the sentence word by word in 3 time steps as above. The words embedding layer is a technique to represent the words ($x_i$) as a vector. There are various word embedding techniques which map (embed) a word into a fixed length vector. In this tutorial we'll use pre-trained embedding vector from [*Conceptnet Numberbatch's (CN)*](https://github.com/commonsense/conceptnet-numberbatch) similar to GloVe, but probably better.

```{r echo=FALSE, out.width="60%", eval=TRUE, fig.cap="Fig 5. Decoder model"}
knitr::include_graphics(path = "assets/decoder-explained.png")
```

Unlike the Encoder LSTM which has the same role to play in both the training phase as well as in the inference phase, the Decoder LSTM has a slightly different role to play in both of these phases. In this section we’ll try to understand how to configure the Decoder during the training phase, while in the next section we’ll understand how to use it during inference.

> The most important point is that the initial states (h0, c0) of the decoder are set to the final states of the encoder. This intuitively means that the decoder is trained to start generating the output sequence depending on the information encoded by the encoder. Obviously the translated Indonesian sentence must depend on the given English sentence.

In the first time step we provide the <sos> or START_ token so that the decoder starts generating the next token (the actual first word of Indonesian sentence). And after the last word in the Indonesian sentence, we make the decoder learn to predict the <eos> or _END token. This will be used as the stopping condition during the inference procedure, basically it will denote the end of the translated sentence and we will stop the inference loop (more on this later). 

We use a technique called “Teacher Forcing” wherein the input at each time step is given as the actual output (and not the predicted output) from the previous time step. This helps in more faster and efficient training of the network (https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/).

---

Inference phase, taken from [this article:](https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7)

a. During inference, we generate one word at a time. Thus the Decoder LSTM is called in a loop, every time processing only one time step.
b. The initial states of the decoder are set to the final states of the encoder.
c. The initial input to the decoder is always the START_ token.
d. At each time step, we preserve the states of the decoder and set them as initial states for the next time step.
e. At each time step, the predicted output is fed as input in the next time step.
f. We break the loop when the decoder predicts the END_ token.

----

Both the encoder and the decoder submodels are trained jointly, meaning at the same time. The entire encoded input is used as context for generating each step in the output. The output sequence relies heavily on the context defined in the final output of the encoder, making it challenging for the model to deal with long sentences. In the case of long sequences, there is a high probability that the initial context has been lost by the end of the sequence. Because it becomes difficult for the encoder to memorize the entire sequence into a fixed-sized vector and to compress all the contextual information from the sequence. As we observed that as the sequence size increases model performance starts getting degrading. An extension of the Encoder-Decoder architecture is to provide a more expressive form of the encoded input sequence and allow the decoder to learn where to pay attention to the encoded input when generating each step of the output sequence. This extension of the architecture is called **attention** [^5].

> *Solution to deal with long sentences:* Bahdanau et al., 2014 and Luong et al., 2015 papers introduced and a technique called “Attention” which allows the model to focus on different parts of the input sequence at every stage of the output sequence allowing the context to be preserved from beginning to end [^9].

So in this attention mechanism, taken from [this article](https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e) give importance to specific parts of the sequence instead of the entire sequence predict that word. Basically, in the attention, we don’t throw away the intermediate from the encoder state but we utilize this to generate context vector from all states so that the decoder gives output result.

Let’s go through an example to understand how attention mechanism works:

* Source sequence: “Which sport do you like the most?

* Target sequence: “I love cricket”

So the idea of attention was to utilize all the contextual information from the input sequence so that we can decode our target sequence. Let’s look at the example we mention. The first word ‘I’ in the target sequence is connected to the fourth word ‘you’ in the source sequence, right? Similarly, the second-word ‘love’ in the target sequence is associated with the fifth word ‘like’ in the source sequence. As we can observe that we are paying more attention to context information like sports, like, you in the source sequence. So, instead of going through the entire sequence it pays attention to specific words from the sequence and gives out the result based on that.


# Implementing Text Summarization using Tensorflow

Since I haven't fineshed yet to translate tensorflow code to R, so the all code below I took from [teamrzaki/text_summarization_abstractive_methods](https://github.com/theamrzaki/text_summurization_abstractive_methods/blob/master/Implementation%20A%20(seq2seq%20with%20attention%20and%20feature%20rich%20representation)/Model_1.ipynb). So we'll build a model text summarization for foods review on amazon. The dataset consists of 500,000 review. 

```{r}
library(reticulate)
```


```{python}
import pandas as pd
import numpy as np
import tensorflow as tf
import re
from nltk.corpus import stopwords
import time
from tensorflow.python.layers.core import Dense
from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors
print('TensorFlow Version: {}'.format(tf.__version__))
```

## Preparing the data

```{python}
reviews = pd.read_csv("Reviews.csv")
reviews.head()
```

Remove null values and unneeded features

```{python}

reviews = reviews.dropna()
reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',
                        'Score','Time'], 1)
reviews = reviews.reset_index(drop=True)
```

Inspecting some of the reviews:

```{python}

for i in range(5):
    print("Review #",i+1)
    print(reviews.Summary[i])
    print(reviews.Text[i])
    print()
```

* 2. Normalizing text data: replace the contraction with the long form.

```{python}
# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python
contractions = { 
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he's": "he is",
"how'd": "how did",
"how'll": "how will",
"how's": "how is",
"i'd": "i would",
"i'll": "i will",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'll": "it will",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"must've": "must have",
"mustn't": "must not",
"needn't": "need not",
"oughtn't": "ought not",
"shan't": "shall not",
"sha'n't": "shall not",
"she'd": "she would",
"she'll": "she will",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"that'd": "that would",
"that's": "that is",
"there'd": "there had",
"there's": "there is",
"they'd": "they would",
"they'll": "they will",
"they're": "they are",
"they've": "they have",
"wasn't": "was not",
"we'd": "we would",
"we'll": "we will",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what will",
"what're": "what are",
"what's": "what is",
"what've": "what have",
"where'd": "where did",
"where's": "where is",
"who'll": "who will",
"who's": "who is",
"won't": "will not",
"wouldn't": "would not",
"you'd": "you would",
"you'll": "you will",
"you're": "you are"
}
```

* 3. Remove unnecessary character and stopwords

We will remove the stopwords from the texts because they do not provide much use for training our model. However, we will keep them for our summaries so that they sound more like natural phrases.

```{python}
def clean_text(text, remove_stopwords = True):
    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''
    
    # Convert words to lower case
    text = text.lower()
    
    # Replace contractions with their longer forms 
    if True:
        text = text.split()
        new_text = []
        for word in text:
            if word in contractions:
                new_text.append(contractions[word])
            else:
                new_text.append(word)
        text = " ".join(new_text)
    
    # Format words and remove unwanted characters
    text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
    text = re.sub(r'\<a href', ' ', text)
    text = re.sub(r'&amp;', '', text) 
    text = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', ' ', text)
    text = re.sub(r'<br />', ' ', text)
    text = re.sub(r'\'', ' ', text)
    
    # Optionally, remove stop words
    if remove_stopwords:
        text = text.split()
        stops = set(stopwords.words("english"))
        text = [w for w in text if not w in stops]
        text = " ".join(text)

    return text
```

```{python}
import nltk
nltk.download('stopwords')
  
# Clean the summaries and texts
clean_summaries = []
for summary in reviews.Summary:
    clean_summaries.append(clean_text(summary, remove_stopwords=False))
print("Summaries are complete.")

clean_texts = []
for text in reviews.Text:
    clean_texts.append(clean_text(text))
print("Texts are complete.")
```

Inspect the cleaned summaries and texts to ensure they have been cleaned well:

```{python}

for i in range(5):
    print("Clean Review #",i+1)
    print(clean_summaries[i])
    print(clean_texts[i])
    print()
```

```{python}
def count_words(count_dict, text):
    '''Count the number of occurrences of each word in a set of text'''
    for sentence in text:
        for word in sentence.split():
            if word not in count_dict:
                count_dict[word] = 1
            else:
                count_dict[word] += 1
```

Find the number of times each word was used and the size of the vocabulary. 

```{python}
word_counts = {}

count_words(word_counts, clean_summaries)
count_words(word_counts, clean_texts)
            
print("Size of Vocabulary:", len(word_counts))
```

Load word embedding model from *Conceptnet Numberbatch's (CN)*, similar to GloVe, but probably better:

```{python}
embeddings_index = {}
with open('numberbatch-en.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split(' ')
        word = values[0]
        embedding = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = embedding

print('Word embeddings:', len(embeddings_index))
```

Find the number of words that are missing from CN, and are used more than our threshold.

```{python}
missing_words = 0
threshold = 20

for word, count in word_counts.items():
    if count > threshold:
        if word not in embeddings_index:
            missing_words += 1
            
missing_ratio = round(missing_words/len(word_counts),4)*100
            
print("Number of words missing from CN:", missing_words)
print("Percent of words that are missing from vocabulary: {}%".format(missing_ratio))
```

Find the number of words that are missing from CN, and are used more than our threshold.

```{python}
missing_words = 0
threshold = 20

for word, count in word_counts.items():
    if count > threshold:
        if word not in embeddings_index:
            missing_words += 1
            
missing_ratio = round(missing_words/len(word_counts),4)*100
            
print("Number of words missing from CN:", missing_words)
print("Percent of words that are missing from vocabulary: {}%".format(missing_ratio))
```

Total number of unique words: 132884
Number of words we will use: 60433
Percent of words we will use: 45.48%

Need to use 300 for embedding dimensions to match CN's vectors.

```{python}
embedding_dim = 300
nb_words = len(vocab_to_int)

# Create matrix with default values of zero
word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)
for word, i in vocab_to_int.items():
    if word in embeddings_index:
        word_embedding_matrix[i] = embeddings_index[word]
    else:
        # If word not in CN, create a random embedding for it
        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))
        embeddings_index[word] = new_embedding
        word_embedding_matrix[i] = new_embedding

# Check if value matches len(vocab_to_int)
print(len(word_embedding_matrix))
```

```{python}
def convert_to_ints(text, word_count, unk_count, eos=False):
    '''Convert words in text to an integer.
       If word is not in vocab_to_int, use UNK's integer.
       Total the number of words and UNKs.
       Add EOS token to the end of texts'''
    ints = []
    for sentence in text:
        sentence_ints = []
        for word in sentence.split():
            word_count += 1
            if word in vocab_to_int:
                sentence_ints.append(vocab_to_int[word])
            else:
                sentence_ints.append(vocab_to_int["<UNK>"])
                unk_count += 1
        if eos:
            sentence_ints.append(vocab_to_int["<EOS>"])
        ints.append(sentence_ints)
    return ints, word_count, unk_count
```

Apply `convert_to_ints` to `clean_summaries` and `clean_texts`

```{python}

word_count = 0
unk_count = 0

int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)
int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)

unk_percent = round(unk_count/word_count,4)*100

print("Total number of words in headlines:", word_count)
print("Total number of UNKs in headlines:", unk_count)
print("Percent of words that are UNK: {}%".format(unk_percent))
```

```{python}
def create_lengths(text):
    '''Create a data frame of the sentence lengths from a text'''
    lengths = []
    for sentence in text:
        lengths.append(len(sentence))
    return pd.DataFrame(lengths, columns=['counts'])
```

```{python}
lengths_summaries = create_lengths(int_summaries)
lengths_texts = create_lengths(int_texts)

print("Summaries:")
print(lengths_summaries.describe())
print()
print("Texts:")
print(lengths_texts.describe())
```

Summaries:
              counts
count  568411.000000
mean        4.181624
std         2.657872
min         0.000000
25%         2.000000
50%         4.000000
75%         5.000000
max        48.000000

Texts:
              counts
count  568411.000000
mean       41.996835
std        42.520873
min         1.000000
25%        18.000000
50%        29.000000
75%        50.000000
max      2085.000000

create a function to remove document review that include to many unknown (UNKs) word.

```{python}
def unk_counter(sentence):
    '''Counts the number of time UNK appears in a sentence.'''
    unk_count = 0
    for word in sentence:
        if word == vocab_to_int["<UNK>"]:
            unk_count += 1
    return unk_count
```

```{python}
sorted_summaries = []
sorted_texts = []
max_text_length = 84
max_summary_length = 13
min_length = 2
unk_text_limit = 1
unk_summary_limit = 0

for length in range(min(lengths_texts.counts), max_text_length): 
    for count, words in enumerate(int_summaries):
        if (len(int_summaries[count]) >= min_length and
            len(int_summaries[count]) <= max_summary_length and
            len(int_texts[count]) >= min_length and
            unk_counter(int_summaries[count]) <= unk_summary_limit and
            unk_counter(int_texts[count]) <= unk_text_limit and
            length == len(int_texts[count])
           ):
            sorted_summaries.append(int_summaries[count])
            sorted_texts.append(int_texts[count])
        
# Compare lengths to ensure they match
print(len(sorted_summaries))
print(len(sorted_texts))
```

## Building the Model

```{python}
def model_inputs():
    '''Create palceholders for inputs to the model'''
    
    input_data = tf.placeholder(tf.int32, [None, None], name='input')
    targets = tf.placeholder(tf.int32, [None, None], name='targets')
    lr = tf.placeholder(tf.float32, name='learning_rate')
    keep_prob = tf.placeholder(tf.float32, name='keep_prob')
    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')
    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')
    text_length = tf.placeholder(tf.int32, (None,), name='text_length')

    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length
```

```{python}
def process_encoding_input(target_data, vocab_to_int, batch_size):
    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''
    
    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])
    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)

    return dec_input
```

```{python}
def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):
    '''Create the encoding layer'''
    
    for layer in range(num_layers):
        with tf.variable_scope('encoder_{}'.format(layer)):
            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,
                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, 
                                                    input_keep_prob = keep_prob)

            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,
                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, 
                                                    input_keep_prob = keep_prob)

            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, 
                                                                    cell_bw, 
                                                                    rnn_inputs,
                                                                    sequence_length,
                                                                    dtype=tf.float32)
    # Join outputs since we are using a bidirectional RNN
    enc_output = tf.concat(enc_output,2)
    
    return enc_output, enc_state
```


```{python}
def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, 
                            vocab_size, max_summary_length):
    '''Create the training logits'''
    
    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,
                                                        sequence_length=summary_length,
                                                        time_major=False)

    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,
                                                       training_helper,
                                                       initial_state,
                                                       output_layer) 

    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,
                                                           output_time_major=False,
                                                           impute_finished=True,
                                                           maximum_iterations=max_summary_length)
    return training_decoder
```

```{python}

def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,
                             max_summary_length, batch_size):
    '''Create the inference logits'''
    
    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')
    
    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,
                                                                start_tokens,
                                                                end_token)
                
    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,
                                                        inference_helper,
                                                        initial_state,
                                                        output_layer)
                
    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,
                                                            output_time_major=False,
                                                            impute_finished=True,
                                                            maximum_iterations=max_summary_length)
    
    return inference_decoder
```

```{python}
def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, 
                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):
    '''Create the decoding cell and attention for the training and inference decoding layers'''
    
    for layer in range(num_layers):
        with tf.variable_scope('decoder_{}'.format(layer)):
            lstm = tf.contrib.rnn.LSTMCell(rnn_size,
                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, 
                                                     input_keep_prob = keep_prob)
    
    output_layer = Dense(vocab_size,
                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))
    
    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,
                                                  enc_output,
                                                  text_length,
                                                  normalize=False,
                                                  name='BahdanauAttention')

    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,
                                                          attn_mech,
                                                          rnn_size)
            
    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],
    #                                                                _zero_state_tensors(rnn_size, 
    #                                                                                    batch_size, 
    #                                                                                    tf.float32)) 
    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])

    with tf.variable_scope("decode"):
        training_decoder = training_decoding_layer(dec_embed_input, 
                                                  summary_length, 
                                                  dec_cell, 
                                                  initial_state,
                                                  output_layer,
                                                  vocab_size, 
                                                  max_summary_length)
        
        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,
                                  output_time_major=False,
                                  impute_finished=True,
                                  maximum_iterations=max_summary_length)
    with tf.variable_scope("decode", reuse=True):
        inference_decoder = inference_decoding_layer(embeddings,  
                                                    vocab_to_int['<GO>'], 
                                                    vocab_to_int['<EOS>'],
                                                    dec_cell, 
                                                    initial_state, 
                                                    output_layer,
                                                    max_summary_length,
                                                    batch_size)
        
        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,
                                  output_time_major=False,
                                  impute_finished=True,
                                  maximum_iterations=max_summary_length)

    return training_logits, inference_logits
```

```{python}
def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, 
                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):
    '''Use the previous functions to create the training and inference logits'''
    
    # Use Numberbatch's embeddings and the newly created ones as our embeddings
    embeddings = word_embedding_matrix
    
    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)
    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)
    
    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)
    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)
    
    training_logits, inference_logits  = decoding_layer(dec_embed_input, 
                                                        embeddings,
                                                        enc_output,
                                                        enc_state, 
                                                        vocab_size, 
                                                        text_length, 
                                                        summary_length, 
                                                        max_summary_length,
                                                        rnn_size, 
                                                        vocab_to_int, 
                                                        keep_prob, 
                                                        batch_size,
                                                        num_layers)
    
    return training_logits, inference_logits
```

```{python}
def pad_sentence_batch(sentence_batch):
    """Pad sentences with <PAD> so that each sentence of a batch has the same length"""
    max_sentence = max([len(sentence) for sentence in sentence_batch])
    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]
```

```{python}
def get_batches(summaries, texts, batch_size):
    """Batch summaries, texts, and the lengths of their sentences together"""
    for batch_i in range(0, len(texts)//batch_size):
        start_i = batch_i * batch_size
        summaries_batch = summaries[start_i:start_i + batch_size]
        texts_batch = texts[start_i:start_i + batch_size]
        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))
        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))
        
        # Need the lengths for the _lengths parameters
        pad_summaries_lengths = []
        for summary in pad_summaries_batch:
            pad_summaries_lengths.append(len(summary))
        
        pad_texts_lengths = []
        for text in pad_texts_batch:
            pad_texts_lengths.append(len(text))
        
        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths
```

Setup hyperparameters:

```{python}
epochs = 100
batch_size = 64
rnn_size = 256
num_layers = 2
learning_rate = 0.005
keep_probability = 0.75
```

```{python}
# Build the graph
train_graph = tf.Graph()
# Set the graph to default to ensure that it is ready for training
with train_graph.as_default():
    
    # Load the model inputs    
    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()

    # Create the training and inference logits
    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),
                                                      targets, 
                                                      keep_prob,   
                                                      text_length,
                                                      summary_length,
                                                      max_summary_length,
                                                      len(vocab_to_int)+1,
                                                      rnn_size, 
                                                      num_layers, 
                                                      vocab_to_int,
                                                      batch_size)
    
    # Create tensors for the training logits and inference logits
    training_logits = tf.identity(training_logits.rnn_output, 'logits')
    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')
    
    # Create the weights for sequence_loss
    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')

    with tf.name_scope("optimization"):
        # Loss function
        cost = tf.contrib.seq2seq.sequence_loss(
            training_logits,
            targets,
            masks)

        # Optimizer
        optimizer = tf.train.AdamOptimizer(learning_rate)

        # Gradient Clipping
        gradients = optimizer.compute_gradients(cost)
        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]
        train_op = optimizer.apply_gradients(capped_gradients)
print("Graph is built.")

```

## Training the Model

Subset the data for training:

```{python}

start = 200000
end = start + 300000
sorted_summaries_short = sorted_summaries[start:end]
sorted_texts_short = sorted_texts[start:end]
print("The shortest text length:", len(sorted_texts_short[0]))
print("The longest text length:",len(sorted_texts_short[-1]))
```



```{python}
# Train the Model
learning_rate_decay = 0.95
min_learning_rate = 0.0005
display_step = 20 # Check training loss after every 20 batches
stop_early = 0 
stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training
per_epoch = 3 # Make 3 update checks per epoch
update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1

update_loss = 0 
batch_loss = 0
summary_update_loss = [] # Record the update losses for saving improvements in the model

  
tf.reset_default_graph()
checkpoint = "drive/Colab Notebooks/Menu/Model 300/best_model.ckpt"  #300k sentence
with tf.Session(graph=train_graph) as sess:
    sess.run(tf.global_variables_initializer())
    
    # If we want to continue training a previous session
    # loader = tf.train.import_meta_graph(checkpoint + '.meta')
    # loader.restore(sess, checkpoint)
    #sess.run(tf.local_variables_initializer())

    for epoch_i in range(1, epochs+1):
        update_loss = 0
        batch_loss = 0
        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(
                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):
            start_time = time.time()
            _, loss = sess.run(
                [train_op, cost],
                {input_data: texts_batch,
                 targets: summaries_batch,
                 lr: learning_rate,
                 summary_length: summaries_lengths,
                 text_length: texts_lengths,
                 keep_prob: keep_probability})

            batch_loss += loss
            update_loss += loss
            end_time = time.time()
            batch_time = end_time - start_time

            if batch_i % display_step == 0 and batch_i > 0:
                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'
                      .format(epoch_i,
                              epochs, 
                              batch_i, 
                              len(sorted_texts_short) // batch_size, 
                              batch_loss / display_step, 
                              batch_time*display_step))
                batch_loss = 0
                
                #saver = tf.train.Saver() 
                #saver.save(sess, checkpoint)
                
            if batch_i % update_check == 0 and batch_i > 0:
                print("Average loss for this update:", round(update_loss/update_check,3))
                summary_update_loss.append(update_loss)
                
              
                  
                # If the update loss is at a new minimum, save the model
                if update_loss <= min(summary_update_loss):
                    print('New Record!') 
                    stop_early = 0
                    saver = tf.train.Saver() 
                    saver.save(sess, checkpoint)

                else:
                    print("No Improvement.")
                    stop_early += 1
                    if stop_early == stop:
                        break
                update_loss = 0
            
                    
        # Reduce learning rate, but not below its minimum value
        learning_rate *= learning_rate_decay
        if learning_rate < min_learning_rate:
            learning_rate = min_learning_rate
        
        if stop_early == stop:
            print("Stopping Training.")
            break
```

## Making our own summaries

To see the quality of the summaries that this model can generate, you can either create your own review, or use a review from the dataset. You can set the length of the summary to a fixed value, or use a random value like I have here.

```{python}
def text_to_seq(text):
    '''Prepare the text for the model'''
    
    text = clean_text(text)
    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]

```

```{python}
random = np.random.randint(0,len(clean_texts))
input_sentence = clean_texts[random]
text = text_to_seq(clean_texts[random])

checkpoint = "drive/Colab Notebooks/Menu/Model 300/best_model.ckpt"

loaded_graph = tf.Graph()
with tf.Session(graph=loaded_graph) as sess:
    # Load saved model
    loader = tf.train.import_meta_graph(checkpoint + '.meta')
    loader.restore(sess, checkpoint)

    input_data = loaded_graph.get_tensor_by_name('input:0')
    logits = loaded_graph.get_tensor_by_name('predictions:0')
    text_length = loaded_graph.get_tensor_by_name('text_length:0')
    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')
    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')
    
    #Multiply by batch_size to match the model's input parameters
    answer_logits = sess.run(logits, {input_data: [text]*batch_size, 
                                      summary_length: [np.random.randint(5,8)], 
                                      text_length: [len(text)]*batch_size,
                                      keep_prob: 1.0})[0] 

# Remove the padding from the tweet
pad = vocab_to_int["<PAD>"] 

print('Original Text:', reviews.Text[random])
print('Original summary:', reviews.Summary[random])#clean_summaries[random]

print('\nText')
print('  Word Ids:    {}'.format([i for i in text]))
print('  Input Words: {}'.format(" ".join([int_to_vocab[i] for i in text])))

print('\nSummary')
print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))
print('  Response Words: {}'.format(" ".join([int_to_vocab[i] for i in answer_logits if i != pad])))

```

* Original Text: This is not only a fabulous tea, but I don't get bored with it.<br />The flavor is interesting and robust enough to keep my attention, but not so intense that it's overwhelming.<br />If you generally like rooibos tea but want it with something extra this is a good choice.<br />They somehow managed to put in the perfect amount of lemon: not so much that it's predominant, but not so little that it's undiscernible.

* Original summary: Really delicious

* generate summary: my favorite tea


## Evaluating Summaries

There are two common metrics for evaluation text summarization result, *bilingual Evaluation Understudy Score* (BLEU) and *Recall-Oriented Understudy for Gisting Evaluation* (ROUGH).

1. BLEU **measures precision**: tells us how much the words (and/ or n-grams) in the machine learning generated summaries appeared in the human reference summaries.
2. Rouge **measures recall**: how much the words (and /  or n-grams) in the human references summaries appeared in the machine generated summaries.
 
## BLEU

The intuition of the BLEU score is we are going to look at the machine generation output and compare to the human references. The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text, where 1-gram or unigram would be each token and a bigram comparison would be each word pair.

> The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation. For this reason, even a human translator will not necessarily score 1. on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references [^8].

We can use the BLEU score for other language generation problems such as:

* Language generation.
* Image caption generation.
* Machine translation.
* Speech recognition.


# References

[^1] Sutskever, I., Vinyals, O., & Le, Q. V. (2014) [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

[^2] Allahyari, M., *et al*. (2017) [Text Summarization Techniques: A brief Survey](https://arxiv.org/pdf/1707.02268.pdf)

[^3] Gaikwad, D. K., Mahender, N. (2016) [A review Paper on Text Summarization](https://pdfs.semanticscholar.org/0681/b372590f7cccc049d24bbddcfb31b68cce61.pdf)

[^4] Cho, Kyunghyun., *et al*. (2014) [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)

[^5] Brownlee, J. (2017) [Encoder-Decoder Models for Text Summarization in Keras](https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/)

[^6] [Understanding Encoder-Decoder Sequence to Sequence Model](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346)

[^7] [A Gentle Introduction to Calculating the BLEU Score for Text](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)

[^8] Papineni, K., *et al* (2002) [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf)

[^9] Dugar, Pranay. (2019) [Attention - Seq2Seq Models](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)

[^10] Lamba, Harshall. (2019) [Word Level English to Marathi Neural Machine Translation using Encoder-Decoder Model](https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7)

[^11] Jangid, A. (2019) [Intuitive Understanding of Seq2seq model & Attention Mechanism in Deep Learning](https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e)